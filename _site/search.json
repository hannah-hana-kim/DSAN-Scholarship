[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DSAN-5100: Introduction",
    "section": "",
    "text": "See the following link for more information about the author: about me\nThis is a Quarto website.\nTo learn more about Quarto websites visit https://quarto.org/docs/websites.\nHIGHLY RECOMMENDED\n\nIt is highly recommended that you build your website using .ipynb files and NOT .qmdfiles\nFunctionally the two formats are basically identical, i.e. they are just Markdown + Code\nHowever there is ONE MAJOR DIFFERENCE, i.e. .ipynb stores the code outputs in the meta-data of the file\n\nThis means you ONLY HAVE TO RUN THE CODE ONCE with .ipynb\n.qmd will run the code every time you build the website, which can be very slow\n\nThere are caching options for .qmd, however, they are “messier” that just using .ipynb\n\nNote: .qmd is fine if there is no code, in which case it is basically just a Markdown file\n\nConverting between the two\n\nYou can switch between the two formats using\nquarto convert clustering.qmd this will output a .ipynb version called clustering.ipynb\nquarto convert eda.ipynb this will output a .qmd version called eda.qmd\n\nYOU CAN RUN R CODE IN VSC WITH .ipynb, see the following link\n\nhttps://saturncloud.io/blog/how-to-use-jupyter-r-kernel-with-visual-studio-code/\n\nIt is possible, but NOT RECOMMENDED, to mix Python and R code in the same file\n\nIMPORTANT ASIDE\n\nA .ipynb file is simply a JSON file with a specialized structural format\nYou can see this by running more eda/eda.ipynb from the command line\nWhich will output the following;\n\n\nTIP FOR MAC USERS\n\ncommand+control+shift+4 is very useful on a mac.\n\nIt takes a screenshot and saves it to the clip-board\n\nThe following VSC extension allows you to paste images from the clip-board with alt+command+v.\n\ntab is your best friend when using the command line, since it does auto-completion\nopen ./path_to_file will open any file or directory from the command line"
  },
  {
    "objectID": "eda/eda.html",
    "href": "eda/eda.html",
    "title": "Data Exploration",
    "section": "",
    "text": "Build out your website tab for exploratory data analysis"
  },
  {
    "objectID": "eda/eda.html#quick-look-at-the-data",
    "href": "eda/eda.html#quick-look-at-the-data",
    "title": "Data Exploration",
    "section": "Quick look at the data",
    "text": "Quick look at the data\n\n# Import seaborn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Apply the default theme\nsns.set_theme(style=\"whitegrid\", palette=\"pastel\")\n\n# Load an example dataset\ntips = sns.load_dataset(\"tips\")\nprint(tips)\n\n     total_bill   tip     sex smoker   day    time  size\n0         16.99  1.01  Female     No   Sun  Dinner     2\n1         10.34  1.66    Male     No   Sun  Dinner     3\n2         21.01  3.50    Male     No   Sun  Dinner     3\n3         23.68  3.31    Male     No   Sun  Dinner     2\n4         24.59  3.61  Female     No   Sun  Dinner     4\n..          ...   ...     ...    ...   ...     ...   ...\n239       29.03  5.92    Male     No   Sat  Dinner     3\n240       27.18  2.00  Female    Yes   Sat  Dinner     2\n241       22.67  2.00    Male    Yes   Sat  Dinner     2\n242       17.82  1.75    Male     No   Sat  Dinner     2\n243       18.78  3.00  Female     No  Thur  Dinner     2\n\n[244 rows x 7 columns]"
  },
  {
    "objectID": "eda/eda.html#basic-visualization",
    "href": "eda/eda.html#basic-visualization",
    "title": "Data Exploration",
    "section": "Basic visualization",
    "text": "Basic visualization\n\n\n# Create a visualization\nsns.relplot(\n    data=tips,\n    x=\"total_bill\", y=\"tip\", col=\"time\",\n    hue=\"smoker\", style=\"smoker\", size=\"size\",\n)\n\nplt.show()"
  },
  {
    "objectID": "clustering/clustering.html",
    "href": "clustering/clustering.html",
    "title": "Clustering",
    "section": "",
    "text": "Build out your website tab for “clustering”"
  },
  {
    "objectID": "data_cleaning_and_viz.html",
    "href": "data_cleaning_and_viz.html",
    "title": "Data Cleaning and Visualization",
    "section": "",
    "text": "# import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import HeatMap\nfrom folium.plugins import HeatMapWithTime, DualMap, TimeSliderChoropleth\nfrom IPython.display import HTML, Javascript, IFrame\nimport branca\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n# load the entire dataset\nnabr = pd.read_csv('./data/NABR_historic.csv')\nnearterm = pd.read_csv('./data/nearterm_data_2020-2024.csv')\n\n\nnabr.head()\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nTimePeriod\nRCP\nscenario\ntreecanopy\nAnn_Herb\nBare\nHerb\n...\nPPT_Annual\nT_Winter\nT_Summer\nT_Annual\nTmax_Summer\nTmin_Winter\nVWC_Winter_whole\nVWC_Spring_whole\nVWC_Summer_whole\nVWC_Fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-12.45\n0.113447\n0.096831\n0.041876\n0.052298\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\nnearterm.head()\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nTimePeriod\nRCP\nscenario\ntreecanopy\nAnn_Herb\nBare\nHerb\n...\nPPT_Annual\nT_Winter\nT_Summer\nT_Annual\nTmax_Summer\nTmin_Winter\nVWC_Winter_whole\nVWC_Spring_whole\nVWC_Summer_whole\nVWC_Fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc22\n0\n0\n84\n5\n...\n6.37\n1.630333\n24.50402\n24.50402\n36.89\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc22\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-12.77\n0.114652\n0.078764\n0.043514\n0.051281\n\n\n2\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc23\n0\n0\n84\n5\n...\n3.09\n1.389056\n24.11043\n24.11043\n37.95\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc23\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-18.96\n0.130221\n0.096412\n0.041232\n0.092241\n\n\n4\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc24\n0\n0\n84\n5\n...\n6.87\n-0.334389\n25.54266\n10.31321\n37.74\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n# lowercase column names\nnabr.columns = nabr.columns.str.lower()\nnearterm.columns = nearterm.columns.str.lower()\n\n# merge the two datasets\ndf = pd.concat([nabr, nearterm], axis=0)\n\n# save the cleaned data to a csv file\ndf.to_csv('./data/cleaned_data/final_data.csv', index=False)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\ntimeperiod\nrcp\nscenario\ntreecanopy\nann_herb\nbare\nherb\n...\nppt_annual\nt_winter\nt_summer\nt_annual\ntmax_summer\ntmin_winter\nvwc_winter_whole\nvwc_spring_whole\nvwc_summer_whole\nvwc_fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-12.45\n0.113447\n0.096831\n0.041876\n0.052298\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\ndf.columns\n\nIndex(['long', 'lat', 'year', 'timeperiod', 'rcp', 'scenario', 'treecanopy',\n       'ann_herb', 'bare', 'herb', 'litter', 'shrub',\n       'drysoildays_summer_whole', 'evap_summer',\n       'extremeshorttermdrystress_summer_whole', 'frostdays_winter',\n       'nondryswa_summer_whole', 'ppt_winter', 'ppt_summer', 'ppt_annual',\n       't_winter', 't_summer', 't_annual', 'tmax_summer', 'tmin_winter',\n       'vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole',\n       'vwc_fall_whole'],\n      dtype='object')\n\n\n\n\n\n\n# Greenery data\n# extract the relevant data\ngreenery_columns = ['long', 'lat', 'year', 'rcp', 'treecanopy', 'bare', 'herb', 'ann_herb', 'litter', 'shrub']\ngreenery = df[greenery_columns]\n\n# calculate the total greenery\ngreenery['total'] = greenery['treecanopy'] + greenery['herb'] + greenery['ann_herb'] + greenery['shrub'] + greenery['litter'] - greenery['bare'] \n\n# save the data to csv file\ngreenery.to_csv('./data/cleaned_data/greenery_data.csv', index=False)\n\n/var/folders/27/cc8xpbnj4vgc0f2m9xmmtc_r0000gn/T/ipykernel_2972/2260858304.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  greenery['total'] = greenery['treecanopy'] + greenery['herb'] + greenery['ann_herb'] + greenery['shrub'] + greenery['litter'] - greenery['bare']\n\n\n\n\n\n\ndf.columns\n\nIndex(['long', 'lat', 'year', 'timeperiod', 'rcp', 'scenario', 'treecanopy',\n       'ann_herb', 'bare', 'herb', 'litter', 'shrub',\n       'drysoildays_summer_whole', 'evap_summer',\n       'extremeshorttermdrystress_summer_whole', 'frostdays_winter',\n       'nondryswa_summer_whole', 'ppt_winter', 'ppt_summer', 'ppt_annual',\n       't_winter', 't_summer', 't_annual', 'tmax_summer', 'tmin_winter',\n       'vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole',\n       'vwc_fall_whole'],\n      dtype='object')\n\n\n\n# Plant litter dataset\n# extract the relevant data\nlitter_columns = ['long', 'lat', 'year', 'rcp', 'litter', 't_winter', 't_summer', 't_annual']\nlitter = df[litter_columns]\nlitter = litter.drop_duplicates(subset=litter_columns)\n\n# # fill the null values with the mean\n# litter.loc[:, 't_winter'] = litter['t_winter'].fillna(litter['t_winter'].mean())\n# litter.loc[:, 't_summer'] = litter['t_summer'].fillna(litter['t_summer'].mean())\n# litter.loc[:, 't_annual'] = litter['t_annual'].fillna(litter['t_annual'].mean())\n\n# drop the null values\nlitter = litter.dropna()\n\n# save the data to csv file\nlitter.to_csv('./data/cleaned_data/litter_data.csv', index=False)\n\nlitter\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nrcp\nlitter\nt_winter\nt_summer\nt_annual\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nhistorical\n11\n0.964835\n23.15924\n23.159240\n\n\n1\n-110.0472\n37.60413\n1980\nhistorical\n11\n0.964835\n23.15924\n0.964835\n\n\n5\n-110.0472\n37.60413\n1981\nhistorical\n11\n3.334444\n23.27065\n11.581320\n\n\n7\n-110.0472\n37.60413\n1982\nhistorical\n11\n-0.015556\n22.05707\n9.472283\n\n\n11\n-110.0472\n37.60413\n1984\nhistorical\n11\n-1.047253\n21.95978\n21.959780\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n55632\n-109.9659\n37.62525\n2024\n8.5\n19\n3.372912\n23.75413\n3.372912\n\n\n55636\n-109.9659\n37.62525\n2024\n8.5\n19\n1.209121\n23.48859\n1.209121\n\n\n55637\n-109.9659\n37.62525\n2024\n8.5\n19\n1.209121\n23.48859\n10.273350\n\n\n55640\n-109.9659\n37.62525\n2024\n8.5\n19\n1.549945\n23.86120\n1.549945\n\n\n55641\n-109.9659\n37.62525\n2024\n8.5\n19\n4.972473\n24.35728\n24.357280\n\n\n\n\n22554 rows × 8 columns\n\n\n\n\n\nlitter.litter.unique()\n\narray([11,  6,  9,  8,  7,  5, 19, 10,  3, 22, 12, 15,  2, 13, 25, 24, 17,\n       18, 21, 26, 20, 16, 14,  1])\n\n\n\n\n\n\n# VWC data\n# extract the relevant data\nvwc_columns = ['long', 'lat', 'year', 'rcp', 'evap_summer', 'vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole', 'vwc_fall_whole']\nvwc = df[vwc_columns]\n\n# deal with NaN values\n# fill the null values with the mean of the column\nnull_columns = ['vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole', 'vwc_fall_whole']\nvwc.loc[:, null_columns] = vwc[null_columns].fillna(vwc[null_columns].mean())\n\n# save it to a new csv file\nvwc.to_csv('./data/cleaned_data/vwc_data.csv', index=False)\n\n\n\n\n\n# summer related data\n# extract the relevant data\nsummer_columns = ['long', 'lat', 'year', 'rcp', 'drysoildays_summer_whole', 'evap_summer', 'nondryswa_summer_whole', 'ppt_summer', 'tmax_summer']\nsummer = df[summer_columns]\n\n# fill missing values with the mean\nsummer.loc[:, 'drysoildays_summer_whole'] = summer['drysoildays_summer_whole'].fillna(summer['drysoildays_summer_whole'].mean())\nsummer.loc[:, 'evap_summer'] = summer['evap_summer'].fillna(summer['evap_summer'].mean())\nsummer.loc[:, 'nondryswa_summer_whole'] = summer['nondryswa_summer_whole'].fillna(summer['nondryswa_summer_whole'].mean())\nsummer.loc[:, 'ppt_summer'] = summer['ppt_summer'].fillna(summer['ppt_summer'].mean())\nsummer.loc[:, 'tmax_summer'] = summer['tmax_summer'].fillna(summer['tmax_summer'].mean())\n\n# round the columns\nsummer = summer.round({'drysoildays_summer_whole': 3, 'evap_summer': 3, 'nondryswa_summer_whole': 3, 'ppt_summer': 3, 'tmax_summer': 3})\n\n# save the data to csv file\nsummer.to_csv('./data/cleaned_data/summer_data.csv', index=False)\n\n\n\n\n\n# summer vs. winter data\n# extract the relevant data\nsummer_winter_columns = ['long', 'lat', 'year', 'rcp', 'ppt_annual', 'ppt_summer', 'ppt_winter', 't_summer', 't_winter', 'tmax_summer', 'tmin_winter']\nsummer_winter = df[summer_winter_columns]\n\n# save the data to csv file\nsummer_winter.to_csv('./data/cleaned_data/summer_winter_data.csv', index=False)"
  },
  {
    "objectID": "data_cleaning_and_viz.html#data-cleaning",
    "href": "data_cleaning_and_viz.html#data-cleaning",
    "title": "Data Cleaning and Visualization",
    "section": "",
    "text": "# import necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport folium\nfrom folium.plugins import HeatMap\nfrom folium.plugins import HeatMapWithTime, DualMap, TimeSliderChoropleth\nfrom IPython.display import HTML, Javascript, IFrame\nimport branca\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom sklearn.preprocessing import MinMaxScaler\nimport warnings \nwarnings.filterwarnings('ignore')\n\n\n# load the entire dataset\nnabr = pd.read_csv('./data/NABR_historic.csv')\nnearterm = pd.read_csv('./data/nearterm_data_2020-2024.csv')\n\n\nnabr.head()\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nTimePeriod\nRCP\nscenario\ntreecanopy\nAnn_Herb\nBare\nHerb\n...\nPPT_Annual\nT_Winter\nT_Summer\nT_Annual\nTmax_Summer\nTmin_Winter\nVWC_Winter_whole\nVWC_Spring_whole\nVWC_Summer_whole\nVWC_Fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-12.45\n0.113447\n0.096831\n0.041876\n0.052298\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\nnearterm.head()\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nTimePeriod\nRCP\nscenario\ntreecanopy\nAnn_Herb\nBare\nHerb\n...\nPPT_Annual\nT_Winter\nT_Summer\nT_Annual\nTmax_Summer\nTmin_Winter\nVWC_Winter_whole\nVWC_Spring_whole\nVWC_Summer_whole\nVWC_Fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc22\n0\n0\n84\n5\n...\n6.37\n1.630333\n24.50402\n24.50402\n36.89\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc22\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-12.77\n0.114652\n0.078764\n0.043514\n0.051281\n\n\n2\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc23\n0\n0\n84\n5\n...\n3.09\n1.389056\n24.11043\n24.11043\n37.95\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc23\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-18.96\n0.130221\n0.096412\n0.041232\n0.092241\n\n\n4\n-110.0472\n37.60413\n2021\nNT\n4.5\nsc24\n0\n0\n84\n5\n...\n6.87\n-0.334389\n25.54266\n10.31321\n37.74\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\n# lowercase column names\nnabr.columns = nabr.columns.str.lower()\nnearterm.columns = nearterm.columns.str.lower()\n\n# merge the two datasets\ndf = pd.concat([nabr, nearterm], axis=0)\n\n# save the cleaned data to a csv file\ndf.to_csv('./data/cleaned_data/final_data.csv', index=False)\n\n\ndf.head()\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\ntimeperiod\nrcp\nscenario\ntreecanopy\nann_herb\nbare\nherb\n...\nppt_annual\nt_winter\nt_summer\nt_annual\ntmax_summer\ntmin_winter\nvwc_winter_whole\nvwc_spring_whole\nvwc_summer_whole\nvwc_fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n1\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n13.79\n0.964835\n23.15924\n0.964835\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n3\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\n2.69\n0.964835\n23.15924\n23.159240\n37.05\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\n-110.0472\n37.60413\n1980\nHist\nhistorical\nsc1\n0\n0\n84\n5\n...\nNaN\nNaN\nNaN\nNaN\nNaN\n-12.45\n0.113447\n0.096831\n0.041876\n0.052298\n\n\n\n\n5 rows × 29 columns\n\n\n\n\n\ndf.columns\n\nIndex(['long', 'lat', 'year', 'timeperiod', 'rcp', 'scenario', 'treecanopy',\n       'ann_herb', 'bare', 'herb', 'litter', 'shrub',\n       'drysoildays_summer_whole', 'evap_summer',\n       'extremeshorttermdrystress_summer_whole', 'frostdays_winter',\n       'nondryswa_summer_whole', 'ppt_winter', 'ppt_summer', 'ppt_annual',\n       't_winter', 't_summer', 't_annual', 'tmax_summer', 'tmin_winter',\n       'vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole',\n       'vwc_fall_whole'],\n      dtype='object')\n\n\n\n\n\n\n# Greenery data\n# extract the relevant data\ngreenery_columns = ['long', 'lat', 'year', 'rcp', 'treecanopy', 'bare', 'herb', 'ann_herb', 'litter', 'shrub']\ngreenery = df[greenery_columns]\n\n# calculate the total greenery\ngreenery['total'] = greenery['treecanopy'] + greenery['herb'] + greenery['ann_herb'] + greenery['shrub'] + greenery['litter'] - greenery['bare'] \n\n# save the data to csv file\ngreenery.to_csv('./data/cleaned_data/greenery_data.csv', index=False)\n\n/var/folders/27/cc8xpbnj4vgc0f2m9xmmtc_r0000gn/T/ipykernel_2972/2260858304.py:7: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  greenery['total'] = greenery['treecanopy'] + greenery['herb'] + greenery['ann_herb'] + greenery['shrub'] + greenery['litter'] - greenery['bare']\n\n\n\n\n\n\ndf.columns\n\nIndex(['long', 'lat', 'year', 'timeperiod', 'rcp', 'scenario', 'treecanopy',\n       'ann_herb', 'bare', 'herb', 'litter', 'shrub',\n       'drysoildays_summer_whole', 'evap_summer',\n       'extremeshorttermdrystress_summer_whole', 'frostdays_winter',\n       'nondryswa_summer_whole', 'ppt_winter', 'ppt_summer', 'ppt_annual',\n       't_winter', 't_summer', 't_annual', 'tmax_summer', 'tmin_winter',\n       'vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole',\n       'vwc_fall_whole'],\n      dtype='object')\n\n\n\n# Plant litter dataset\n# extract the relevant data\nlitter_columns = ['long', 'lat', 'year', 'rcp', 'litter', 't_winter', 't_summer', 't_annual']\nlitter = df[litter_columns]\nlitter = litter.drop_duplicates(subset=litter_columns)\n\n# # fill the null values with the mean\n# litter.loc[:, 't_winter'] = litter['t_winter'].fillna(litter['t_winter'].mean())\n# litter.loc[:, 't_summer'] = litter['t_summer'].fillna(litter['t_summer'].mean())\n# litter.loc[:, 't_annual'] = litter['t_annual'].fillna(litter['t_annual'].mean())\n\n# drop the null values\nlitter = litter.dropna()\n\n# save the data to csv file\nlitter.to_csv('./data/cleaned_data/litter_data.csv', index=False)\n\nlitter\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nrcp\nlitter\nt_winter\nt_summer\nt_annual\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nhistorical\n11\n0.964835\n23.15924\n23.159240\n\n\n1\n-110.0472\n37.60413\n1980\nhistorical\n11\n0.964835\n23.15924\n0.964835\n\n\n5\n-110.0472\n37.60413\n1981\nhistorical\n11\n3.334444\n23.27065\n11.581320\n\n\n7\n-110.0472\n37.60413\n1982\nhistorical\n11\n-0.015556\n22.05707\n9.472283\n\n\n11\n-110.0472\n37.60413\n1984\nhistorical\n11\n-1.047253\n21.95978\n21.959780\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n55632\n-109.9659\n37.62525\n2024\n8.5\n19\n3.372912\n23.75413\n3.372912\n\n\n55636\n-109.9659\n37.62525\n2024\n8.5\n19\n1.209121\n23.48859\n1.209121\n\n\n55637\n-109.9659\n37.62525\n2024\n8.5\n19\n1.209121\n23.48859\n10.273350\n\n\n55640\n-109.9659\n37.62525\n2024\n8.5\n19\n1.549945\n23.86120\n1.549945\n\n\n55641\n-109.9659\n37.62525\n2024\n8.5\n19\n4.972473\n24.35728\n24.357280\n\n\n\n\n22554 rows × 8 columns\n\n\n\n\n\nlitter.litter.unique()\n\narray([11,  6,  9,  8,  7,  5, 19, 10,  3, 22, 12, 15,  2, 13, 25, 24, 17,\n       18, 21, 26, 20, 16, 14,  1])\n\n\n\n\n\n\n# VWC data\n# extract the relevant data\nvwc_columns = ['long', 'lat', 'year', 'rcp', 'evap_summer', 'vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole', 'vwc_fall_whole']\nvwc = df[vwc_columns]\n\n# deal with NaN values\n# fill the null values with the mean of the column\nnull_columns = ['vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole', 'vwc_fall_whole']\nvwc.loc[:, null_columns] = vwc[null_columns].fillna(vwc[null_columns].mean())\n\n# save it to a new csv file\nvwc.to_csv('./data/cleaned_data/vwc_data.csv', index=False)\n\n\n\n\n\n# summer related data\n# extract the relevant data\nsummer_columns = ['long', 'lat', 'year', 'rcp', 'drysoildays_summer_whole', 'evap_summer', 'nondryswa_summer_whole', 'ppt_summer', 'tmax_summer']\nsummer = df[summer_columns]\n\n# fill missing values with the mean\nsummer.loc[:, 'drysoildays_summer_whole'] = summer['drysoildays_summer_whole'].fillna(summer['drysoildays_summer_whole'].mean())\nsummer.loc[:, 'evap_summer'] = summer['evap_summer'].fillna(summer['evap_summer'].mean())\nsummer.loc[:, 'nondryswa_summer_whole'] = summer['nondryswa_summer_whole'].fillna(summer['nondryswa_summer_whole'].mean())\nsummer.loc[:, 'ppt_summer'] = summer['ppt_summer'].fillna(summer['ppt_summer'].mean())\nsummer.loc[:, 'tmax_summer'] = summer['tmax_summer'].fillna(summer['tmax_summer'].mean())\n\n# round the columns\nsummer = summer.round({'drysoildays_summer_whole': 3, 'evap_summer': 3, 'nondryswa_summer_whole': 3, 'ppt_summer': 3, 'tmax_summer': 3})\n\n# save the data to csv file\nsummer.to_csv('./data/cleaned_data/summer_data.csv', index=False)\n\n\n\n\n\n# summer vs. winter data\n# extract the relevant data\nsummer_winter_columns = ['long', 'lat', 'year', 'rcp', 'ppt_annual', 'ppt_summer', 'ppt_winter', 't_summer', 't_winter', 'tmax_summer', 'tmin_winter']\nsummer_winter = df[summer_winter_columns]\n\n# save the data to csv file\nsummer_winter.to_csv('./data/cleaned_data/summer_winter_data.csv', index=False)"
  },
  {
    "objectID": "data_cleaning_and_viz.html#visualization",
    "href": "data_cleaning_and_viz.html#visualization",
    "title": "Data Cleaning and Visualization",
    "section": "Visualization",
    "text": "Visualization\n\nGreenery area dataset\n\n# load the data\ngreenery = pd.read_csv('./data/cleaned_data/greenery_data.csv')\ngreenery\n\n/var/folders/27/cc8xpbnj4vgc0f2m9xmmtc_r0000gn/T/ipykernel_2972/4013972958.py:2: DtypeWarning:\n\nColumns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nrcp\ntreecanopy\nbare\nherb\nann_herb\nlitter\nshrub\ntotal\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nhistorical\n0\n84\n5\n0\n11\n7\n-61\n\n\n1\n-110.0472\n37.60413\n1980\nhistorical\n0\n84\n5\n0\n11\n7\n-61\n\n\n2\n-110.0472\n37.60413\n1980\nhistorical\n0\n84\n5\n0\n11\n7\n-61\n\n\n3\n-110.0472\n37.60413\n1980\nhistorical\n0\n84\n5\n0\n11\n7\n-61\n\n\n4\n-110.0472\n37.60413\n1980\nhistorical\n0\n84\n5\n0\n11\n7\n-61\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69618\n-109.9659\n37.62525\n2024\n8.5\n18\n37\n18\n0\n19\n22\n40\n\n\n69619\n-109.9659\n37.62525\n2024\n8.5\n18\n37\n18\n0\n19\n22\n40\n\n\n69620\n-109.9659\n37.62525\n2024\n8.5\n18\n37\n18\n0\n19\n22\n40\n\n\n69621\n-109.9659\n37.62525\n2024\n8.5\n18\n37\n18\n0\n19\n22\n40\n\n\n69622\n-109.9659\n37.62525\n2024\n8.5\n18\n37\n18\n0\n19\n22\n40\n\n\n\n\n69623 rows × 11 columns\n\n\n\n\n\ngreenery1 = greenery.groupby(['long', 'lat'])['total'].mean().reset_index()\ngreenery1\n\n\n\n\n\n\n\n\n\nlong\nlat\ntotal\n\n\n\n\n0\n-110.0472\n37.60413\n-61.0\n\n\n1\n-110.0461\n37.60334\n-48.0\n\n\n2\n-110.0458\n37.60308\n-29.0\n\n\n3\n-110.0393\n37.60519\n-15.0\n\n\n4\n-110.0389\n37.60545\n-25.0\n\n\n...\n...\n...\n...\n\n\n108\n-109.9673\n37.62868\n-30.0\n\n\n109\n-109.9669\n37.62446\n-91.0\n\n\n110\n-109.9662\n37.60783\n-4.0\n\n\n111\n-109.9662\n37.63159\n50.0\n\n\n112\n-109.9659\n37.62525\n40.0\n\n\n\n\n113 rows × 3 columns\n\n\n\n\n\n# further data processing\ngreenery_total = greenery.groupby(['long', 'lat', 'year'])['total'].mean().reset_index()\ngreenery_total\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\ntotal\n\n\n\n\n0\n-110.0472\n37.60413\n1980\n-61.0\n\n\n1\n-110.0472\n37.60413\n1981\n-61.0\n\n\n2\n-110.0472\n37.60413\n1982\n-61.0\n\n\n3\n-110.0472\n37.60413\n1983\n-61.0\n\n\n4\n-110.0472\n37.60413\n1984\n-61.0\n\n\n...\n...\n...\n...\n...\n\n\n4854\n-109.9659\n37.62525\n2018\n40.0\n\n\n4855\n-109.9659\n37.62525\n2021\n40.0\n\n\n4856\n-109.9659\n37.62525\n2022\n40.0\n\n\n4857\n-109.9659\n37.62525\n2023\n40.0\n\n\n4858\n-109.9659\n37.62525\n2024\n40.0\n\n\n\n\n4859 rows × 4 columns\n\n\n\n\n\n# create a folium heatmap of the 1980 data\nmap_center = [greenery_total['lat'].mean(), greenery_total['long'].mean()]\nmap_width = 800  \nmap_height = 600 \nmymap_1980 = folium.Map(location=map_center, zoom_start=13, width=map_width, height=map_height)\n\nheat_data_green = data_1980[['lat', 'long', 'total']].reset_index(drop=True).values.tolist()\n\n# the higher the gradient, the more greenery there is. \nHeatMap(heat_data_green, radius=20, blur=10, gradient={0.2: 'red', 0.4: 'brown', 0.6: 'yellow', 0.8: '#32CD32', 1: 'green'}).add_to(mymap_1980)\n\nmymap_1980\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# create a folium heatmap of the 1980 data\nmap_center = [greenery_total['lat'].mean(), greenery_total['long'].mean()]\nmap_width = 800  \nmap_height = 600 \nmymap_2024 = folium.Map(location=map_center, zoom_start=13, width=map_width, height=map_height)\n\nheat_data_green = data_2024[['lat', 'long', 'total']].reset_index(drop=True).values.tolist()\n\n# the higher the gradient, the more greenery there is. \nHeatMap(heat_data_green, radius=20, blur=10, gradient={0.2: 'red', 0.4: 'brown', 0.6: 'yellow', 0.8: '#32CD32', 1: 'green'}).add_to(mymap_2024)\n\nmymap_2024\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# create a folium heatmap\nmap_center = [greenery_total['lat'].mean(), greenery_total['long'].mean()]\nmap_width = 800  \nmap_height = 600 \nmymap = folium.Map(location=map_center, zoom_start=13, width=map_width, height=map_height)\n\nheat_data_green = greenery_total[['lat', 'long', 'total']].reset_index(drop=True).values.tolist()\n\n\n# the higher the gradient, the more greenery there is. \nHeatMap(heat_data_green, radius=20, blur=10, gradient={0.2: 'red', 0.4: 'brown', 0.6: 'yellow', 0.8: '#32CD32', 1: 'green'}).add_to(mymap)\n\nmymap\n\nMake this Notebook Trusted to load map: File -&gt; Trust Notebook\n\n\n\n# consider year; slide the slider to see the changes in greenery over the years\ngreenery_year = greenery.groupby(['long', 'lat', 'year'])['total'].mean().reset_index()\n\n\nstyledict = {\n    str(year): {\n        'radius': 20,\n        'blur': 10,\n        'gradient': {0.2: 'red', 0.4: 'brown', 0.6: 'yellow', 0.8: '#32CD32', 1: 'green'},\n        'data': greenery_year[year]\n    } for year in greenery_year.keys()\n}\n\nmymap2 = folium.Map(location=map_center, zoom_start=13, width=map_width, height=map_height)\n\nTimeSliderChoropleth(styledict).add_to(mymap2)\n\nmymap2\n\nTypeError: TimeSliderChoropleth.__init__() missing 1 required positional argument: 'styledict'\n\n\n\n\nPlant litter dataset\n\n# load the data\nlitter = pd.read_csv('./data/cleaned_data/litter_data.csv')\n\n\n# further data processing\nlitter_copy = litter.copy()\n# drop long and lat and rcp columns\nlitter_copy = litter_copy.drop(columns=['long', 'lat', 'rcp'], axis=1)\n\n# group by year and get the mean of the columns\nlitter_copy = litter_copy.groupby('year', as_index=False).agg({'litter': lambda x: x.mode().iloc[0], \n                                     't_winter': 'mean', \n                                     't_summer': 'mean', \n                                     't_annual': 'mean'})\n\n# round the columns\nlitter_copy = litter_copy.round({'t_winter': 3, 't_summer': 3, 't_annual': 3})\n\nlitter_copy.head()\n\n\n\n\n\n\n\n\n\nyear\nlitter\nt_winter\nt_summer\nt_annual\n\n\n\n\n0\n1980\n9\n0.773\n22.816\n10.849\n\n\n1\n1981\n9\n3.239\n22.874\n10.993\n\n\n2\n1982\n6\n-0.201\n21.620\n11.135\n\n\n3\n1983\n10\n0.243\n20.795\n9.601\n\n\n4\n1984\n6\n-1.258\n21.462\n9.281\n\n\n\n\n\n\n\n\n\nlitter_copy.litter.unique()\n\narray([ 9,  6, 10,  8])\n\n\n\nlitter_copy_2 = litter.copy()\n# drop long and lat and rcp columns\nlitter_copy_2 = litter_copy_2.drop(columns=['long', 'lat', 'rcp', 'year'], axis=1)\n\n# group by litter column\nlitter_copy_2 = litter_copy_2.groupby('litter', as_index=False).agg({'t_winter': 'mean', \n                                     't_summer': 'mean', \n                                     't_annual': 'mean'})\n\n# round the columns\nlitter_copy_2 = litter_copy_2.round({'t_winter': 3, 't_summer': 3, 't_annual': 3})\n\nlitter_copy_2.head()\n\n\n\n\n\n\n\n\n\nlitter\nt_winter\nt_summer\nt_annual\n\n\n\n\n0\n1\n0.669\n23.375\n11.700\n\n\n1\n2\n1.068\n23.999\n12.444\n\n\n2\n3\n1.015\n23.804\n12.093\n\n\n3\n5\n0.737\n23.440\n11.589\n\n\n4\n6\n0.900\n23.800\n12.057\n\n\n\n\n\n\n\n\n\nlitter_copy_2.litter.unique()\n\narray([ 1,  2,  3,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17, 18,\n       19, 20, 21, 22, 24, 25, 26])\n\n\n\nfig = px.scatter(litter_copy_2, x='litter', y='t_annual', trendline = 'ols', title='Annual Temperature vs. Litter')\nregression_line = fig.data[1]\nfig.add_trace(go.Scatter(x=regression_line.x, y=regression_line.y, mode='lines', name='Trendline', line = dict(color='red')))\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nVWC dataset\n\n# load the data\nvwc = pd.read_csv('./data/cleaned_data/vwc_data.csv')\nvwc.head()\n\n/var/folders/27/cc8xpbnj4vgc0f2m9xmmtc_r0000gn/T/ipykernel_2972/3121339849.py:2: DtypeWarning:\n\nColumns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nrcp\nevap_summer\nvwc_winter_whole\nvwc_spring_whole\nvwc_summer_whole\nvwc_fall_whole\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nhistorical\nNaN\n0.128169\n0.112395\n0.069182\n0.095330\n\n\n1\n-110.0472\n37.60413\n1980\nhistorical\nNaN\n0.128169\n0.112395\n0.069182\n0.095330\n\n\n2\n-110.0472\n37.60413\n1980\nhistorical\nNaN\n0.128169\n0.112395\n0.069182\n0.095330\n\n\n3\n-110.0472\n37.60413\n1980\nhistorical\nNaN\n0.128169\n0.112395\n0.069182\n0.095330\n\n\n4\n-110.0472\n37.60413\n1980\nhistorical\n1.559807\n0.113447\n0.096831\n0.041876\n0.052298\n\n\n\n\n\n\n\n\n\n# further data cleaning\nvwc_melt = pd.melt(vwc, id_vars=['long', 'lat', 'year', 'rcp'], value_vars=['vwc_winter_whole', 'vwc_spring_whole', 'vwc_summer_whole', 'vwc_fall_whole'], var_name='season', value_name='vwc')\nvwc_grouped = vwc_melt.groupby(['year', 'rcp', 'season']).agg({'vwc': 'mean'}).reset_index()\nvwc_grouped = vwc_grouped.round({'vwc': 3})\nvwc_grouped['rcp'] = vwc_grouped['rcp'].astype('str')\nvwc_grouped.head()\nvwc_grouped.to_csv('./data/cleaned_data/vwc_grouped.csv', index=False)\n\n\nvwc_grouped\n\n\n\n\n\n\n\n\n\nyear\nrcp\nseason\nvwc\n\n\n\n\n0\n1980\nhistorical\nvwc_fall_whole\n0.090\n\n\n1\n1980\nhistorical\nvwc_spring_whole\n0.120\n\n\n2\n1980\nhistorical\nvwc_summer_whole\n0.069\n\n\n3\n1980\nhistorical\nvwc_winter_whole\n0.131\n\n\n4\n1981\nhistorical\nvwc_fall_whole\n0.102\n\n\n...\n...\n...\n...\n...\n\n\n215\n2024\n4.5\nvwc_winter_whole\n0.130\n\n\n216\n2024\n8.5\nvwc_fall_whole\n0.096\n\n\n217\n2024\n8.5\nvwc_spring_whole\n0.111\n\n\n218\n2024\n8.5\nvwc_summer_whole\n0.068\n\n\n219\n2024\n8.5\nvwc_winter_whole\n0.129\n\n\n\n\n220 rows × 4 columns\n\n\n\n\n\nplt.figure(figsize=(10, 6))\nsns.barplot(data=vwc_grouped, x=\"vwc\", y=\"season\", hue=\"rcp\", orient=\"h\")\nplt.xlabel(\"VWC\")\nplt.ylabel(\"Season\")\nplt.title(\"Volumetric Water Content (VWC) by Season\")\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot out relatiionship between VWC and evaporation and temperature\n\n\n\nSummer dataset\n\nFirst Visualization\n\n# load the data\nsummer = pd.read_csv('./data/cleaned_data/summer_data.csv')\n\n# further data processing\nsummer_copy = summer.copy()\nsummer_copy = summer.drop(['long', 'lat', 'rcp'], axis=1)\n\nsummer_copy = summer_copy.groupby(['year']).mean().reset_index()\n\n# convert the year column to string type\nsummer_copy['year'] = summer_copy['year'].astype('str')\n\nsummer_copy.to_csv('./data/cleaned_data/summer_data_2.csv', index=True)\nsummer_copy.head()\n\n/var/folders/27/cc8xpbnj4vgc0f2m9xmmtc_r0000gn/T/ipykernel_2972/1897514567.py:2: DtypeWarning:\n\nColumns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n\n\n\n\n\n\n\n\n\n\n\nyear\ndrysoildays_summer_whole\nevap_summer\nnondryswa_summer_whole\nppt_summer\ntmax_summer\n\n\n\n\n0\n1980\n7.856000\n2.876526\n0.271869\n3.992827\n36.821592\n\n\n1\n1981\n6.880705\n4.098247\n0.213244\n9.247074\n37.152125\n\n\n2\n1982\n7.139391\n3.789916\n0.298272\n9.448209\n36.545367\n\n\n3\n1983\n6.439958\n4.049732\n0.311255\n10.045459\n34.954063\n\n\n4\n1984\n6.791979\n4.043963\n0.246205\n9.134468\n35.494011\n\n\n\n\n\n\n\n\n\n# plot a correlation matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(summer_copy.corr(), annot=True, cmap='coolwarm')\nplt.title('Correlation Matrix')\nplt.show()\n\n\n\n\n\n\n\n\n\n# plot a line graph of drysoildays_summer_whole, evap_summer, nondryswa_summer_whole, ppt_summer, and tmax_summer\nfig = px.line(summer_copy, x='year', y=['drysoildays_summer_whole', 'evap_summer', 'ppt_summer'], \n              hover_name='year', template='simple_white', color_discrete_sequence=[\"#EED21B\", \"#CA68C8\", \"#A7C6DA\"])\n\nfig.for_each_trace(lambda trace: trace.update(name = trace.name.replace('drysoildays_summer_whole', 'Days of Dry Soil (&lt; -3.9 MPa)')\n                                                      .replace('evap_summer', 'Evaporation')\n                                                      .replace('ppt_summer', 'Precipitation')))\n\nfig.update_traces(mode = 'markers+lines', hovertemplate=None)\nfig.update_layout(title='Something of the Summer Data', xaxis_title='Year', yaxis_title='Values', hovermode = 'x', legend_title='Summer Indicators')\n\nfig.show()\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nSecond Visualization\n\n# spider chart of summer data\nsummer_copy_2 = summer.copy()\nsummer_copy_2\n\n\n\n\n\n\n\n\n\nlong\nlat\nyear\nrcp\ndrysoildays_summer_whole\nevap_summer\nnondryswa_summer_whole\nppt_summer\ntmax_summer\n\n\n\n\n0\n-110.0472\n37.60413\n1980\nhistorical\n7.368\n3.446\n0.285\n2.69\n37.050\n\n\n1\n-110.0472\n37.60413\n1980\nhistorical\n7.368\n3.446\n0.285\n2.69\n37.050\n\n\n2\n-110.0472\n37.60413\n1980\nhistorical\n7.368\n3.446\n0.285\n2.69\n37.050\n\n\n3\n-110.0472\n37.60413\n1980\nhistorical\n7.368\n3.446\n0.285\n2.69\n37.050\n\n\n4\n-110.0472\n37.60413\n1980\nhistorical\n0.000\n1.560\n0.032\n7.44\n37.104\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n69618\n-109.9659\n37.62525\n2024\n8.5\n0.000\n3.557\n0.552\n7.44\n37.104\n\n\n69619\n-109.9659\n37.62525\n2024\n8.5\n0.000\n2.398\n0.107\n7.44\n37.104\n\n\n69620\n-109.9659\n37.62525\n2024\n8.5\n0.000\n3.000\n0.595\n7.44\n37.104\n\n\n69621\n-109.9659\n37.62525\n2024\n8.5\n0.000\n3.702\n0.121\n7.44\n37.104\n\n\n69622\n-109.9659\n37.62525\n2024\n8.5\n0.000\n2.450\n0.075\n7.44\n37.104\n\n\n\n\n69623 rows × 9 columns\n\n\n\n\n\n# drop unnecessary columns\nsummer_copy_2 = summer_copy_2.drop(['long', 'lat', 'rcp'], axis=1)\n\n\nsummer_mean = summer_copy_2.groupby(['year']).mean().reset_index()\n# normalize each column\nscaler = MinMaxScaler()\nnormalization = ['drysoildays_summer_whole', 'evap_summer', 'nondryswa_summer_whole', 'ppt_summer', 'tmax_summer']\nsummer_norm = summer_mean.copy()\nsummer_norm[normalization] = scaler.fit_transform(summer_norm[normalization])\n\n# save the normalized data to a csv file\nsummer_norm.to_csv('./data/cleaned_data/summer_data_normalized.csv', index=False)\n\nsummer_norm\n\n\n\n\n\n\n\n\n\nyear\ndrysoildays_summer_whole\nevap_summer\nnondryswa_summer_whole\nppt_summer\ntmax_summer\n\n\n\n\n0\n1980\n0.514474\n0.000000\n0.416921\n0.000000\n0.684300\n\n\n1\n1981\n0.255970\n0.832378\n0.101002\n0.577175\n0.792399\n\n\n2\n1982\n0.324536\n0.622308\n0.559199\n0.599269\n0.593962\n\n\n3\n1983\n0.139149\n0.799324\n0.629162\n0.664877\n0.073535\n\n\n4\n1984\n0.232453\n0.795393\n0.278620\n0.564805\n0.250122\n\n\n5\n1985\n0.326470\n0.088161\n0.436613\n0.190075\n0.779893\n\n\n6\n1986\n0.291818\n0.664557\n0.376677\n0.614292\n0.222361\n\n\n7\n1987\n0.204933\n0.604053\n0.833924\n0.733650\n0.000000\n\n\n8\n1988\n0.329069\n0.650974\n0.265041\n0.502173\n0.336115\n\n\n9\n1989\n0.342523\n0.357455\n0.222188\n0.354591\n0.926105\n\n\n10\n1990\n0.832580\n0.322382\n0.025741\n0.243596\n0.700367\n\n\n11\n1991\n0.401208\n0.534754\n0.188000\n0.340979\n0.086735\n\n\n12\n1992\n0.326372\n0.491797\n0.388490\n0.288700\n0.053028\n\n\n13\n1993\n0.411210\n0.195792\n0.505235\n0.287138\n0.182061\n\n\n14\n1994\n0.592861\n0.110479\n0.072140\n0.022122\n0.706431\n\n\n15\n1995\n0.325735\n0.599439\n0.402182\n0.373839\n0.707837\n\n\n16\n1996\n1.000000\n0.378491\n0.007566\n0.215613\n0.455155\n\n\n17\n1997\n0.157649\n0.476947\n0.854368\n0.519582\n0.314864\n\n\n18\n1998\n0.358906\n0.364233\n0.289358\n0.291193\n0.681136\n\n\n19\n1999\n0.102077\n1.000000\n0.989318\n1.000000\n0.020302\n\n\n20\n2000\n0.629187\n0.331719\n0.000000\n0.211229\n0.621852\n\n\n21\n2001\n0.193425\n0.563343\n0.357521\n0.472579\n0.452666\n\n\n22\n2002\n0.925616\n0.162072\n0.067557\n0.055366\n0.680658\n\n\n23\n2003\n0.416787\n0.354054\n0.141522\n0.281033\n0.869549\n\n\n24\n2004\n0.628803\n0.085185\n0.097432\n0.031600\n0.256525\n\n\n25\n2005\n0.187448\n0.511368\n0.600632\n0.462813\n0.939569\n\n\n26\n2006\n0.400429\n0.561612\n0.159993\n0.409084\n0.633954\n\n\n27\n2007\n0.366045\n0.475055\n0.219222\n0.420195\n0.483594\n\n\n28\n2008\n0.280460\n0.391333\n0.340532\n0.355953\n0.248358\n\n\n29\n2009\n0.502573\n0.069051\n0.067321\n0.028907\n0.490793\n\n\n30\n2010\n0.173548\n0.595272\n1.000000\n0.640504\n0.816138\n\n\n31\n2011\n0.428323\n0.503414\n0.065421\n0.269703\n0.396243\n\n\n32\n2012\n0.448498\n0.284454\n0.291205\n0.348593\n0.359680\n\n\n33\n2013\n0.507180\n0.308917\n0.379229\n0.463412\n1.000000\n\n\n34\n2014\n0.300057\n0.401213\n0.353896\n0.374638\n0.400486\n\n\n35\n2015\n0.000000\n0.865975\n0.723682\n0.823589\n0.498817\n\n\n36\n2016\n0.290868\n0.443416\n0.360286\n0.392252\n0.853709\n\n\n37\n2017\n0.305248\n0.332105\n0.163788\n0.329793\n0.762924\n\n\n38\n2018\n0.867626\n0.093284\n0.053515\n0.177406\n0.365574\n\n\n39\n2021\n0.373643\n0.402562\n0.463269\n0.379992\n0.721086\n\n\n40\n2022\n0.399668\n0.350936\n0.404685\n0.341678\n0.916108\n\n\n41\n2023\n0.344075\n0.448684\n0.662350\n0.458952\n0.806748\n\n\n42\n2024\n0.416539\n0.295430\n0.562352\n0.329676\n0.933750\n\n\n\n\n\n\n\n\n\ndisplay(Javascript(\"\"\"\n    &lt;div class=\"flourish-embed flourish-radar\" data-src=\"visualisation/17966848\"&gt;&lt;script src=\"https://public.flourish.studio/resources/embed.js\"&gt;&lt;/script&gt;&lt;/div&gt;\n\"\"\"))"
  }
]